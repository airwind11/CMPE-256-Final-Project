{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from pylab import rcParams\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.externals import joblib\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_folder_path= r\"../Data\"\n",
    "    #filename1 = \"/WalmartDatasset.txt\"\n",
    "    filename1 = \"/nov01-30days.txt\"\n",
    "    filename2 = \"/nov21-30days.txt\"\n",
    "    \n",
    "    header_names=[\"term\",\"product_id\", \"language\",\"product_impressions\",\"Product_clicks\",\"cart_adds\",\"cart_start\",\"checkout\",\"order\"]\n",
    "    df_dly_hdp = pd.read_csv(input_folder_path+filename1, sep='\\t', names = header_names)\n",
    "    df_dly_hdp1 = pd.read_csv(input_folder_path+filename2, sep='\\t', names = header_names)\n",
    "    frames = [df_dly_hdp,df_dly_hdp1]\n",
    "    result = pd.concat(frames,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # filter out French language records\n",
    "    result = (result[result['language'] == 'English'])\n",
    "\n",
    "    joined_df_dly = (\n",
    "    result[['term', 'product_id', 'product_impressions', 'Product_clicks', 'cart_adds', 'checkout', 'order']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_dly_hdp)\n",
    "\n",
    "\n",
    "joined_df_dly['searchTerm'] = joined_df_dly['term']\n",
    "joined_df_dly['ATR'] = joined_df_dly['cart_adds'] / joined_df_dly['product_impressions']\n",
    "joined_df_dly['CTR'] = joined_df_dly['Product_clicks'] / joined_df_dly['product_impressions']\n",
    "joined_df_dly['conv'] = joined_df_dly['order'] / joined_df_dly['product_impressions']\n",
    "joined_df_dly['ATRmCTR'] = joined_df_dly['ATR'] * joined_df_dly['CTR']\n",
    "\n",
    "columns_all = ['searchTerm', 'product_id', 'product_impressions', 'Product_clicks', 'cart_adds', 'order', 'CTR', 'ATR',\n",
    "               'conv','ATRmCTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3045860 entries, 0 to 3045859\n",
      "Data columns (total 11 columns):\n",
      "index                  int64\n",
      "searchTerm             object\n",
      "product_id             object\n",
      "product_impressions    int64\n",
      "Product_clicks         int64\n",
      "cart_adds              int64\n",
      "order                  int64\n",
      "CTR                    float64\n",
      "ATR                    float64\n",
      "conv                   float64\n",
      "ATRmCTR                float64\n",
      "dtypes: float64(4), int64(5), object(2)\n",
      "memory usage: 255.6+ MB\n",
      "None\n",
      "       searchTerm     product_id  product_impressions  Product_clicks  \\\n",
      "0    men t shirts  6000196965847                    7               1   \n",
      "1     dvi to hdmi  6000066423242                   65              11   \n",
      "2  rubix cube 4x4  6000196692502                    3               1   \n",
      "3         Day bed  6000195505799                   10               1   \n",
      "4           razor  6000195571001                    9               1   \n",
      "\n",
      "   cart_adds  order       CTR       ATR  conv   ATRmCTR  \n",
      "0          0      0  0.142857  0.000000   0.0  0.000000  \n",
      "1          2      0  0.169231  0.030769   0.0  0.005207  \n",
      "2          0      0  0.333333  0.000000   0.0  0.000000  \n",
      "3          0      0  0.100000  0.000000   0.0  0.000000  \n",
      "4          1      0  0.111111  0.111111   0.0  0.012346  \n"
     ]
    }
   ],
   "source": [
    "# columns for input\n",
    "\n",
    "joined_df_dly = joined_df_dly[list(columns_all)].reset_index()\n",
    "\n",
    "print(joined_df_dly.info())\n",
    "\n",
    "###########################################3\n",
    "\n",
    "# print(joined_df_dly.tail(50))\n",
    "\n",
    "# Concat null results and positive results - Final step\n",
    "\n",
    "# columns for input\n",
    "\n",
    "final_merged_dataset = joined_df_dly[list(columns_all)]\n",
    "final_merged_dataset = final_merged_dataset.replace(np.nan, 0)\n",
    "final_merged_dataset = final_merged_dataset.drop_duplicates()\n",
    "\n",
    "final_merged_dataset.to_csv(r\"/Users/soumyamohapatra/CMPE-256/Data/final_merged_dataset_mthly2_test.csv\", sep=',',\n",
    "                            index=False)\n",
    "\n",
    "# result = df_dly_null.append(df_dly_pos, ignore_index=True)\n",
    "print(final_merged_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('final_merged_dataset_mthly2.csv')\n",
    "df2 = pd.read_csv('../Data/final_merged_dataset_mthly2_test.csv', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "##filter value which are not zero , zero values will not aid prediction\n",
    "df3 = df2[((df2['ATR'] >0)&(df2['CTR'] >0)&(df2['conv'] >0))]\n",
    "#df3 = df2[df2['conv'] >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_counts = {}\n",
    "for i in df3['searchTerm']:\n",
    "    if i in row_counts:\n",
    "        row_counts[i]+=1\n",
    "    else:\n",
    "        row_counts[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51364"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(row_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "highfreq_searchStrings = []\n",
    "for j in row_counts:\n",
    "    if row_counts[j] > 50:\n",
    "        highfreq_searchStrings.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "highfreq_searchStrings = [x for x in highfreq_searchStrings if not any(c.isdigit() for c in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat food\n"
     ]
    }
   ],
   "source": [
    "print (highfreq_searchStrings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data cleaning as few outliers were present and it was difficult to visualize data relationship , very fat tail\n",
    "##lot of value in the lower end \n",
    "def removeoutliers(highfreq_searchStrings= None):\n",
    "    if highfreq_searchStrings == None:\n",
    "        dfsearch = df3\n",
    "    else:\n",
    "        dfsearch = df3[(df3['searchTerm'] == highfreq_searchStrings)]\n",
    "    ATRstd,CTRstd = dfsearch[['ATR','CTR']].std()\n",
    "    ATRmean,CTRmean = dfsearch[['ATR','CTR']].mean()\n",
    "    dfsearchWoOutliers = dfsearch[((dfsearch['ATR']<=(ATRmean+2*ATRstd))& (dfsearch['ATR']>=(ATRmean-1*ATRstd)))]\n",
    "    dfsearchWoOutliers = dfsearchWoOutliers[((dfsearch['CTR']<=(CTRmean+2*CTRstd))& (dfsearch['CTR']>=(CTRmean-1*CTRstd)))]\n",
    "    return dfsearchWoOutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplotgraphs(dfsearchWoOutliers):\n",
    "    rcParams['figure.figsize'] = 5,5\n",
    "    sb.set_style('whitegrid')\n",
    "    #plt.hist(dfsearch['CTR'])\n",
    "    #plt.plot()\n",
    "    sb.pairplot(dfsearchWoOutliers,vars=['CTR','ATR','ATRmCTR','conv'], y_vars=['conv'])\n",
    "    dfsearchWoOutliers.plot(kind='scatter',x='ATR',y='conv',c=['darkgray'],s=150)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(dfsearchWoOutliers['CTR'], dfsearchWoOutliers['ATR'], dfsearchWoOutliers['conv'], c='r', marker='o')\n",
    "    ax.set_xlabel('CTR')\n",
    "    ax.set_ylabel('ATR')\n",
    "    ax.set_zlabel('CONV')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#X_train,X_test,y_train,y_test = train_test_split(dfsearchWoOutliers[['CTR']],dfsearchWoOutliers[['conv']],test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abc=[]\n",
    "def Regressors(dfsearchWoOutliers):\n",
    "    mean_scores =[]\n",
    "    reg0 = LinearRegression()\n",
    "    reg0.fit(dfsearchWoOutliers[['CTR']],dfsearchWoOutliers[['conv']])\n",
    "    scores0 = cross_val_score(reg0, dfsearchWoOutliers[['CTR']], dfsearchWoOutliers[['conv']])\n",
    "    #print(reg0.coef_)\n",
    "    #print(reg0.intercept_)\n",
    "    #print (\"with CTR\",scores0.mean())\n",
    "    mean_scores.append(scores0.mean())\n",
    "    abc.append(reg0)\n",
    "    \n",
    "    \n",
    "    reg1 = LinearRegression()\n",
    "    reg1.fit(dfsearchWoOutliers[['ATR']],dfsearchWoOutliers[['conv']])\n",
    "    scores1 = cross_val_score(reg1, dfsearchWoOutliers[['ATR']], dfsearchWoOutliers[['conv']])\n",
    "    #print (reg1.coef_)\n",
    "    #print (reg1.intercept_)\n",
    "    #print (\"with ATR\",scores1.mean())\n",
    "    mean_scores.append(scores1.mean())\n",
    "    \n",
    "    reg2 = LinearRegression()\n",
    "    reg2.fit(dfsearchWoOutliers[['CTR','ATR']],dfsearchWoOutliers[['conv']])\n",
    "    scores2 = cross_val_score(reg2, dfsearchWoOutliers[['CTR','ATR']], dfsearchWoOutliers[['conv']])\n",
    "    #results=reg2.predict(df2[['CTR','ATR']])\n",
    "    #print (results)\n",
    "    #print (reg2.coef_)\n",
    "    #print (reg2.intercept_)\n",
    "    #print (\"with ATRandCTR\",scores2.mean())\n",
    "    mean_scores.append(scores2.mean())\n",
    "    \n",
    "    reg3 = LinearRegression()\n",
    "    reg3.fit(dfsearchWoOutliers[['CTR','ATR','ATRmCTR']],dfsearchWoOutliers[['conv']])\n",
    "    scores3 = cross_val_score(reg3, dfsearchWoOutliers[['CTR','ATR','ATRmCTR']], dfsearchWoOutliers[['conv']])\n",
    "    #print (reg4.coef_)\n",
    "    #print (reg4.intercept_)\n",
    "    #print (\"with ATR,CTRandInteraction\",scores3.mean())\n",
    "    mean_scores.append(scores3.mean())\n",
    "    \n",
    "    poly2 = PolynomialFeatures(degree=2)\n",
    "    X2_Poly = poly2.fit_transform(dfsearchWoOutliers[['CTR','ATR']])\n",
    "    reg4 = LinearRegression()\n",
    "    reg4.fit(X2_Poly,dfsearchWoOutliers[['conv']])\n",
    "    scores4 = cross_val_score(reg4, X2_Poly, dfsearchWoOutliers[['conv']])\n",
    "    #print (reg5.coef_)\n",
    "    #print (reg5.intercept_)\n",
    "    #print (\"with ATR,CTR2ndDegree\",scores4.mean())\n",
    "    mean_scores.append(scores4.mean())\n",
    "    \n",
    "    ##To validate if scaling is changing model parameters\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X2_Poly)\n",
    "    scaler.transform(X2_Poly)\n",
    "    reg41 = LinearRegression()\n",
    "    reg41.fit(X2_Poly,dfsearchWoOutliers[['conv']])\n",
    "    scores7 = cross_val_score(reg41, X2_Poly, dfsearchWoOutliers[['conv']])\n",
    "    #print (reg41.coef_)\n",
    "    #print (reg41.intercept_)\n",
    "    scores41 = cross_val_score(reg41, X2_Poly, dfsearchWoOutliers[['conv']])\n",
    "    #print (\"with Scaled ATR,CTR2ndDegree\",scores41.mean())\n",
    "    \n",
    "    \n",
    "    poly3 = PolynomialFeatures(degree=3)\n",
    "    X3_Poly = poly3.fit_transform(dfsearchWoOutliers[['CTR','ATR']])\n",
    "    reg5 = LinearRegression()\n",
    "    reg5.fit(X3_Poly,dfsearchWoOutliers[['conv']])\n",
    "    scores5 = cross_val_score(reg5, X3_Poly, dfsearchWoOutliers[['conv']])\n",
    "    #print (reg5.coef_)\n",
    "    #print (reg5.intercept_)\n",
    "    #print (\"with ATR,CTR3rdDegree\",scores5.mean())\n",
    "    mean_scores.append(scores5.mean())\n",
    "    \n",
    "    neigh = KNeighborsRegressor(n_neighbors=10)\n",
    "    neigh.fit(dfsearchWoOutliers[['CTR','ATR']],dfsearchWoOutliers[['conv']])\n",
    "    scores6 = cross_val_score(neigh, dfsearchWoOutliers[['CTR','ATR']], dfsearchWoOutliers[['conv']])                                                                   \n",
    "    #print (\"Neighbour\",scores6.mean())\n",
    "    mean_scores.append(scores6.mean())\n",
    "    \n",
    "    reg7 = RandomForestRegressor()\n",
    "    reg7.fit(dfsearchWoOutliers[['CTR']],dfsearchWoOutliers[['conv']])\n",
    "    scores7 = cross_val_score(reg7, dfsearchWoOutliers[['CTR']], dfsearchWoOutliers[['conv']])\n",
    "    #print (\"with randomforest\",scores7.mean())\n",
    "    mean_scores.append(scores7.mean())\n",
    "    \n",
    "    counter_meanscores = list(enumerate(mean_scores, 1))\n",
    "    counter_list_sorted = sorted(counter_meanscores, key=lambda x:x[1],reverse=True)\n",
    "    #print (counter_list_sorted[0][1])\n",
    "    \n",
    "    if counter_list_sorted[0][1]>=.2:\n",
    "        if counter_list_sorted[0][0]==1:\n",
    "            return reg0,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "        elif counter_list_sorted[0][0]==2:\n",
    "            return reg1,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "        elif counter_list_sorted[0][0]==3:\n",
    "            return reg2,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "        elif counter_list_sorted[0][0]==4:\n",
    "            return reg3,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "        elif counter_list_sorted[0][0]==5:\n",
    "            return reg4,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "        elif counter_list_sorted[0][0]==6:\n",
    "            return reg5,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "        elif counter_list_sorted[0][0]==7:\n",
    "            return neigh,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "        elif counter_list_sorted[0][0]==8:\n",
    "            return reg7,counter_list_sorted[0][1],counter_list_sorted[0][0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for single dimension input\n",
    "def plotregresmodel(model,datax,datay,datainlist):\n",
    "    plt.scatter(datax,datay)\n",
    "    plt.plot(datax,model.predict(datainlist),color='blue',linewidth=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "##no default model as global data doesn't have any trend \n",
    "#def modelordefault(searchstring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def predict(ctr,atr,searchstring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat food\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) R-squarer 0.24386816843 model-num 2\n",
      "open LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n"
     ]
    }
   ],
   "source": [
    "dfsearchWoOutliers = removeoutliers(\"canadiana\")\n",
    "print (highfreq_searchStrings[2])\n",
    "model,r2,number = Regressors(dfsearchWoOutliers)\n",
    "print (model,\"R-squarer\",r2,\"model-num\",number)\n",
    "joblib_file_name = ('trial'+'.joblib')\n",
    "fileObject = open('./pickle/'+joblib_file_name,'wb')\n",
    "joblib.dump(model, fileObject)\n",
    "fileObject.close()\n",
    "#models[searchstring] = [joblib_file_name,r2,number]\n",
    "#joblib.dumps(models)\n",
    "fileObject = open('./pickle/'+joblib_file_name,'rb')\n",
    "b = joblib.load(fileObject)\n",
    "print (\"open\",b)\n",
    "#pairplotgraphs(dfsearchWoOutliers)\n",
    "#plotregresmodel(abc[0],dfsearchWoOutliers['CTR'],dfsearchWoOutliers['conv'],dfsearchWoOutliers[['CTR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for global model\n",
    "#dfsearchWoOutliers = removeoutliers()\n",
    "#Regressors(dfsearchWoOutliers)\n",
    "#pairplotgraphs(dfsearchWoOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={}\n",
    "for searchstring in highfreq_searchStrings:\n",
    "    #print (searchstring)\n",
    "    dfsearchWoOutliers = removeoutliers(searchstring)\n",
    "    if Regressors(dfsearchWoOutliers)!=None:\n",
    "        model,r2,number = Regressors(dfsearchWoOutliers)\n",
    "        name = ''.join(x for x in searchstring if x.isalpha())\n",
    "        #print (name)\n",
    "        joblib_file_name = (name+'.joblib')\n",
    "        #print (joblib_file_name)\n",
    "        fileObject = open('./pickle/'+joblib_file_name,'wb')\n",
    "        joblib.dump(model, fileObject)\n",
    "        fileObject.close()\n",
    "        models[searchstring] = [joblib_file_name,r2,number]                   \n",
    "        #pairplotgraphs(dfsearchWoOutliers)\n",
    "        #print (model,r2,number)\n",
    "fileObject = open(\"./pickle/models.joblib\",'wb')\n",
    "joblib.dump(models, fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileObject = open(\"./pickle/initial-dataframe.joblib\",'wb')\n",
    "#joblib.dump(df2, fileObject)\n",
    "#fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommendtop10products if string present in model and top 10 rule is that if the predicted value of conversion \n",
    "#is higher than existing value then add it to a list with the conversion values and then sort the list based on\n",
    "#conversion values and show the top 10\n",
    "\n",
    "def recommendtop10products():\n",
    "    recommendations={}\n",
    "    fileObject = open(\"./pickle/models.joblib\",'rb')\n",
    "    model_dict = joblib.load(fileObject)\n",
    "    #fileObject = open(\"./pickle/initial-dataframe.joblib\",'rb')\n",
    "    #initial_dataframe = joblib.load(fileObject)\n",
    "    #print (\"model\",model_dict)\n",
    "    for keys,values in model_dict.items():\n",
    "        df=df2[(df2['searchTerm'] == keys)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        #print (df.loc[0])\n",
    "        #print (df[['CTR','ATR']])\n",
    "        print (keys,values)\n",
    "        fileObject1 = open(('./pickle/'+values[0]),'rb')\n",
    "        regmodel = joblib.load(fileObject1)\n",
    "        #print (regmodel)\n",
    "        if values[2]==1:\n",
    "            results=regmodel.predict(df[['CTR']])\n",
    "        elif values[2]==2:\n",
    "            results = regmodel.predict(df[['ATR']])\n",
    "        elif values[2]==3:\n",
    "            results = regmodel.predict(df[['CTR','ATR']])\n",
    "        elif values[2]==4:\n",
    "            results = regmodel.predict(df[['CTR','ATR','ATRmCTR']])\n",
    "        elif values[2]==5:\n",
    "            poly2 = PolynomialFeatures(degree=2)\n",
    "            X2_Poly = poly2.fit_transform(df[['CTR','ATR']])\n",
    "            results = regmodel.predict(X2_Poly)\n",
    "        elif values[2]==6:\n",
    "            poly3 = PolynomialFeatures(degree=3)\n",
    "            X3_Poly = poly3.fit_transform(df[['CTR','ATR']])\n",
    "            results = regmodel.predict(X3_Poly)\n",
    "        elif values[2]==7:\n",
    "            results = regmodel.predict(df[['CTR','ATR']])\n",
    "        elif values[2]==8:\n",
    "            results = regmodel.predict(df[['CTR']])\n",
    "            \n",
    "        results = results.tolist()\n",
    "        results = list(enumerate(results,0))\n",
    "        results = sorted(results, key=lambda x:x[1],reverse=True)\n",
    "        ## take top 10 results from the sorted enumerated results and use the position to get the products from \n",
    "        #dataframe\n",
    "        productlist=[]\n",
    "        for i in range(10):\n",
    "            productlist.append(df.loc[results[i][0]]['product_id'])\n",
    "        recommendations[keys]=[productlist]\n",
    "        fileObject1.close()\n",
    "    fileObject.close()\n",
    "    return recommendations       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommendtop10products()\n",
    "print (recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
